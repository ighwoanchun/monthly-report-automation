"""Gemini APIë¥¼ ì‚¬ìš©í•œ ë¦¬í¬íŠ¸ ìƒì„± (ë¬´ë£Œ)"""
import google.generativeai as genai
from pathlib import Path
from .config import Config


class AIReportGenerator:
    def __init__(self):
        genai.configure(api_key=Config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
        self.prompt_template = self._load_prompt()

    def _load_prompt(self) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
        prompt_path = Path(__file__).parent.parent / "prompts" / "analysis_prompt.md"
        if prompt_path.exists():
            return prompt_path.read_text(encoding="utf-8")
        return self._default_prompt()

    def _default_prompt(self) -> str:
        return """
ë‹¹ì‹ ì€ ì±„ìš© í”Œë«í¼ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ìš©ì–´ ì •ì˜
- pass_cnt = ì„œë¥˜í†µê³¼ ìˆ˜ (í•©ê²© ì•„ë‹˜)
- hire_cnt = í•©ê²© ìˆ˜ (ìµœì¢… í•©ê²©)
- new_com_accept = ì‹ ê·œê¸°ì—… ê°€ì…
- recruit_fee = ìˆ˜ìˆ˜ë£Œ ë§¤ì¶œ

## ë°ì´í„° êµ¬ë¶„
- í•©ê²©ê¸°ì¤€ë¦¬ë“œíƒ€ì„_raw: "í•´ë‹¹ ì›”ì— í•©ê²©í•œ ì‚¬ëŒë“¤" â†’ Part A ì‹¤ì  ë¶„ì„
- ì§€ì›ê¸°ì¤€ë¦¬ë“œíƒ€ì„_raw: "í•´ë‹¹ ì›”ì— ì§€ì›í•œ ì‚¬ëŒë“¤" â†’ Part B ì˜ˆì¸¡ ë¶„ì„

## ì „í™˜ìœ¨ ì°¸ì¡°
ì§€ì›â†’í•©ê²©: ë‹¹ì›” 11.3%, ì „ì›” 45.2%â˜…, ì „ì „ì›” 25.8%
ì„œë¥˜í†µê³¼â†’í•©ê²©: ë‹¹ì›” 26%, ì „ì›” 37.5%â˜…, ì „ì „ì›” 14%

## ë¦¬í¬íŠ¸ êµ¬ì¡°
### Part A. ì‹¤ì  ë¶„ì„ (í•©ê²©ê¸°ì¤€)
1. Executive Summary
2. ì›”ë³„ KPI ì¶”ì´
3. ë§¤ì¶œ êµ¬ì¡°
4. í•©ê²©ì ë¶„ì„ (ì§êµ°ë³„/ê¸°ì—…ê·œëª¨ë³„/ë¦¬ë“œíƒ€ì„)
5. ì„±ê³¼ í‰ê°€

### Part B. íŒŒì´í”„ë¼ì¸ ë¶„ì„ (ì§€ì›ê¸°ì¤€)
6. ì§€ì› í˜„í™©
7. í¼ë„ ì „í™˜ ë¶„ì„
8. ìµì›” ì˜ˆì¸¡

### ê³µí†µ
9. ë¦¬ìŠ¤í¬ & ê¸°íšŒ
10. ì•¡ì…˜ ì•„ì´í…œ

## ì£¼ì˜ì‚¬í•­
- ì§êµ°ë³„ í•©ê³„ëŠ” ì›”í†µí•©ë¶„ì„ì˜ hire_cntì™€ ì¼ì¹˜í•´ì•¼ í•¨
- TOP 10 ì™¸ ì§êµ°ì€ "ê¸°íƒ€"ë¡œ ë¬¶ê³  êµ¬ì„± ëª…ì‹œ
- ë¯¸ë¶„ë¥˜(null) ë°ì´í„°ëŠ” "ë¯¸ë¶„ë¥˜"ë¡œ í‘œì‹œ

ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

    def generate_report(self, data_context: str) -> str:
        """ì „ì²´ ë¦¬í¬íŠ¸ ìƒì„±"""
        prompt = f"{self.prompt_template}\n\n## ë¶„ì„ ë°ì´í„°\n{data_context}"

        response = self.model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=8000,
                temperature=0.3,
            ),
        )

        return response.text

    def generate_summary(self, full_report: str) -> str:
        """Slackìš© ìš”ì•½ ìƒì„±"""
        prompt = f"""
ë‹¤ìŒ ë¦¬í¬íŠ¸ì—ì„œ Slack ê³µìœ ìš© Executive Summaryë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[í¬ë§·]
*í•µì‹¬ ì„±ê³¼*
â€¢ ì´ ë§¤ì¶œ: â‚©XX.Xì–µ (Â±X.X% MoM)
â€¢ í•©ê²© ìˆ˜: XXXê±´ (Â±X.X% MoM)
â€¢ ì‹ ê·œê¸°ì—…: XXXê±´ (Â±X.X% MoM)

*âš ï¸ Alert* (1-2ê°œ)
*ğŸ“ˆ Opportunity* (1-2ê°œ)

---
ë¦¬í¬íŠ¸ ë‚´ìš©:
{full_report[:4000]}
"""
        response = self.model.generate_content(prompt)
        return response.text
